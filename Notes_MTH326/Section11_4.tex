\setSection{3}
\section{Properties of the Least-Squares Estimators: Simple Linear Regression}

\nl The Coefficents of the Best-Fit Line are Estimators. We have shown the best-fit line to be $\displaystyle \widehat{y} = \widehat{\beta_0} + \widehat{\beta_1}x$.

\nl Where $\displaystyle \widehat{\beta_1} = \dfrac{S_{xy}}{S_{xx}}$ with $S_{xy} = \sum (x_i - \bar{x})(y_i - \bar{y})$ and $S_{xx} = \sum (x_i - \bar{x})^2$. (i.e. $\Cov(X,Y)$ and $\Vb{X}$ via use of $\bar{x}$, $\bar{y}$ for $\mu_x$, $\mu_y$ ).

\nl $\displaystyle \widehat{\beta_0} = \bar{y} - \widehat{\beta_1}x$

\nl We recognize $\widehat{\beta_0}$ and $\widehat{\beta_1}$ as stats dependent upon $\bar{x}, S_x^2,$ and $\bar{y}$.


\nnl But what exactly are they estimating? In theory, $X \times Y$ distributed via pdf $f(x,y)$ and there is some \say{best} linear relationship over the same probabilty space. 
$$y = \beta_0 + \beta_1 x.$$

\nl In other words, when we are \say{at} $x_i$,
$$\Eb{Y} = \beta_0 + \beta_1 x_i.$$

\nl A modern approach to the distribution of $Y$ at $x$ is to add an error parameter $\red{\varepsilon}$. That is,
$$\setGreen y = \setBlack \underbrace{\setGreen \beta_0 + \beta_1 x}_{\substack{ \text{deterministic}\\\text{component}\\\text{of } Y}}
+ \underbrace{\red{\varepsilon}}_{\substack{\text{\say{random}}\\ \text{component}}}$$

\nl Still want $\Eb{Y} = \beta_0 + \beta_1 x$. By linearity,

\begin{align*}
    \Eb{Y} &= \Eb{\beta_0 + \beta_1x + \varepsilon}\\
    &= \Eb{\beta_0 + \beta_1x} + \Eb{\varepsilon}\\
    &\implies \Eb{\varepsilon} = 0 \quad \text{error averages to zero}
\end{align*}

\nl We make the additional assumption that the variance of $\varepsilon$ is independent of $x$. That is $\Vb{Y} = \Vb{\varepsilon} = \sigma^2$. The \say{value} of $y$ \textit{depends} on $x$, but the spread of the $y$-values does not.

\nnl\textbf{Proposition:} $\widehat{\beta_0}, \widehat{\beta_1}$ are unbiased estimators of $\beta_0, \beta_1$ where $Y = \beta_0 + \beta_1x + \varepsilon$.

\nl \textbf{Reason:} 
\begin{align*}
    \E*{\widehat{\beta_1}} &= \Eb{\frac{S_{xy}}{S_{xx}}}\\
    &= \Eb{\frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{S_{xx}}}\\
    &= \Eb{\frac{\sum (x_i - \bar{x})y_i - \sum (x_i - \bar{x})\bar{y}}{S_{xx}}}\\
    &= \Eb{\frac{\sum(x_i - \bar{x})y_i - \overbrace{\bar{y} \sum(x_i - \bar{x})}^{0 \text{ by def'n of } \bar{x}}}{S_{xx}}}\\
    &= \Eb{\frac{\sum (x_i - \bar{x})y_i}{S_{xx}}}\\
    &= \frac{\sum (x_i - \bar{x}) \E*{y_i}}{S_{xx}}\\
    &= \frac{\sum (x_i - \bar{x}) \setGreen{(\beta_0 + \beta_1 x_i)}}{S_{xx}}\\
    &= \frac{\overbrace{\beta_0 \sum (x_i - \bar{x})}^{0} + \beta_1 \sum (x_i - \bar{x})x_i}{S_{xx}}\\
    &= \frac{\beta_1 \sum (x_i^2 - x_i \bar{x})}{S_{xx}}\\
    &= \frac{\beta_1 \pars{\sum x_i^2 - \bar{x} \sum x_i}}{S_{xx}}\\
    &= \frac{\beta_1 \pars{\sum x_i^2 - n\bar{x}^2}}{S_{xx}}\\
    &= \beta_1 \frac{S_{xx}}{S_{xx}}\\
    &= \beta_1
\end{align*}
Therefore our estimator is unbiased.

\nl The point of all that: $\E*{\widehat{\beta_1}} = \widehat{\beta_1}$.

\nl For $\E*{\widehat{\beta_0}} = \Eb{\bar{y} -\widehat{\beta_1} \bar{x}} = \Eb{\bar{y}} - \bar{x} \E*{\widehat{\beta_1}}$.

\nl But $\displaystyle \bar{y} = \over{n} \sum y_i = \over{n} \sum (\beta_0 + \beta_1 x + \varepsilon) = \beta_0 + \beta_1 \bar{x} + \varepsilon$.

\nl and $\Eb{\bar{y}} = \beta_0 + \beta_1 \bar{x}$. and $\Eb{\widehat{\beta_0}} = \beta_0 + \beta_1 \bar{x} - \beta_1 \bar{x} = \beta_0$.


\nnl \textbf{Corollary:} $\displaystyle \Var*{\widehat{\beta_1}} = \dfrac{\sigma^2}{S_{XX}}, \Var*{\widehat{\beta_0}} = \frac{\sigma^2 \sum x_i^2}{nS_{XX}} $.

\nl \textbf{Reason} 
\begin{align*}
    \Var*{\widehat{\beta_1}} &= \Var{\dfrac{\sum(x_i-\bar{x})(y_i - \bar{y})}{\sum(x_i-\bar{x})^2}}\\
    &= \Var{\dfrac{\sum(x_i-\bar x)^2y_i}{S_{XX}}}\\
    &= \over{{S_{XX}}^2} \Var{\sum (x_i-\bar x)y_i}\\
    &= \frac{\sum (x_i - \bar x)^2 \Var{Y_i}}{{S_{XX}}^2}
    \\ \text{assumption: } \Var{Y} = \Var{\varepsilon} = \sigma^2\\
    &= \frac{\sum (x_i - \bar x)^2 \sigma^2}{{S_{XX}}^2}\\
    &= \frac{\sigma^2 \sum(x_i - \bar x)^2}{{S_{XX}}^2}\\
    &= \frac{\sigma^2 S_{XX}}{{S_{XX}}^2} \\
    &= \frac{\sigma^2}{S_{XX}}.
\end{align*}

\begin{align*}
    \Var*{\widehat{\beta_0}} &= \Var{\bar y - \widehat{\beta_1} \bar x}\\
    \text{Note in our } \varepsilon & \text{ set up, } \bar y \text{ and } \widehat{\beta_1} \text{ both functions of } Y_i\text{'s\dots may be independent.}
    \\ &= \Var {\bar y} - \Var*{\widehat{\beta_1} \bar x} - 2 \Cov (\bar y, \widehat{\beta_1}\bar x)\\
    &= \Var{\bar y} + \bar x^2 \Var*{\widehat{\beta_1}} - 2\bar x^2 \Cov (\bar y, \widehat{\beta_1})
\end{align*}
And i.) $\displaystyle \Var{\bar y} = \Var{\bar{\varepsilon}} = \over{n} \Var{\varepsilon} = \over{x} \sigma^2$.
And ii.)
\begin{align*}
    \Cov (\bar y, \widehat{\beta_1}) &= \Cov \pars{\over{n}\sum y_i,\; \sum \frac{(x_i - \bar x)y_i}{S_{XX}}}\\
    &= \sum \frac{(x_i - \bar x) \Var{Y_i}}{nS_{xx}} + \sum \sum_{i \neq j} \frac{(x_j 0 \bar x)}{n S_{XX}} \underbrace{\Cov (Y_i, Y_j)}_{\text{independent } = 0}\\
    &= \frac{\sigma^2}{n S_{XX}} \sum (x_i - \bar x)\\
    &= 0.
\end{align*}
Then 
\begin{align*}
    \Var*{\widehat{\beta_0}} &= \Var{\bar y} + \bar x^2  \Var*{\widehat{\beta_1}} - 2 \bar x^2 \Cov (\bar y, \widehat{\beta_1})\\
    &= \frac{\sigma^2}{n} + \bar x^2 \cdot \frac{\sigma^2}{S_{XX}}\\
    &= \sigma^2 \pfrac{S_{XX}+n \bar x^2}{nS_{XX}}
\end{align*}

\begin{align*}
    S_{XX} &= \sum (x_i - \bar x)^2\\
    &= \sum(\bar x^2 - 2 \bar x) + \sum {x_i}^2\\
    &= n \bar x^2 - 2\bar x \sum x_i\\
    &= n \bar x^2 - 2 \bar x n \pfrac{\sum x_i}{n} + \sum {x_i}^2\\
    &= \sum {x_i}^2 - n \bar x^2 \qquad \text{i.e. } n \bar x^2 = \sum {x_i}^2 - S_{XX}
\end{align*}

\begin{align*}
    \Var*{\widehat{\beta_0}} &= \sigma^2 \pfrac{S_{XX} + \sum {x_i}^2 - S_{XX}}{n S_{XX}}\\
    &= \frac{\sigma^2 \sum {x_i}^2}{n S_{XX}}
\end{align*}

\nl Corollary: $\Cov (\widehat{\beta_0}, \widehat{\beta_1}) = - \frac{\bar x \sigma^2}{S_{XX}}$. Proof in textbook. Note: $\widehat{\beta_0}, \widehat{\beta_1}$ guaranteed to be dependent when $\bar x=0$.

\nnl \textbf{Topic: } estimating $\sigma^2$.

\nl We have working with the assumption that $\Var y = \Var{\varepsilon} = \sigma^2$, but this is usually unknown.

\nl In the past, we used $\displaystyle \Var Y = \over{n-1} \sum (y_i - \bar y)^2$.

\nl In our new setup, our point estimator for $y_i$ is no longer $\bar y$. Our estimator of $Y_i$ is the best-fit line: $\E{Y_i} = \widehat{\beta_0} + \widehat{\beta_1}x.$

\nl We define $S^2$ via sum square error
$$\operatorname{SSE} = \sum_{i=1}^n (y_i, \widehat{y_i})^2.$$
The term inside the summation is the residual; it is the error between the prediction and observed.
%residual pic

\nl We need to define an unbiased estimator for $\sigma^2$. Let 
\begin{align*}
    \that &:= K \cdot \operatorname{SSE} \\
    &= K \sum (y_i - (\widehat{\beta_0} + \widehat{\beta_1}x_i))^2
\end{align*}
By computing $\E*{\that}$, we choose $K$ so that $\E*{\that} = \sigma^2.$

\nl As we did in chapter 9, we can show that $\E*{\operatorname{SSE}} = (n-2) \sigma^2$. So, 
$$\boxed{S^2 = \over{n-2} \operatorname{SSE} = \over{n-2} \sum_{i=1}^n (y_i - \widehat{y_i})^2}$$

\nnl \textbf{Proposition: } $\E*{S^2} = \sigma^2$, (unbiased estimator).\\(aside: It would be nice to have a computation corollary like we did, i.e. $\Var{x} = \E{x^2} - \E x^2$)

\nl \textbf{Corollary: } The computation corollary
$$\operatorname{SSE} = S_{YY} - \widehat{\beta_1} S_{XY}$$
$$\text{where} \quad S_{YY} = \sum_{i=1}^n (y_i - \bar y)^2.$$

\example (11.16)\\
Potnecy of antibiotic ($y$): $(38,43,29), (32,26,33), (19,27,27), (14,19,21)$
\\After storage at $x^{\circ}$F: $30, 50, 70, 90$

$$\bar y = 27, \qquad \bar x = 60$$
$$S_{xy} = \sum (x_i-\bar x)(y_i i \bar y) = -1900$$
$$S_{xx} = \sum (x_i - \bar x)^2 = 6000$$
$$S_{yy} = \sum (y_i - \bar y)^2 = 792$$
Then $\widehat{\beta_1}= \dfrac{S_{xy}}{S_{xx}} = -\dfrac{19}{60}$ and $$\widehat{\beta_0} = \bar y - \widehat{\beta_1} \Xbar x = 27 - \pfrac*{-19}{60}60 = 46.$$
So the best fit line is $\widehat y = 46 - \frac{19}{60}x$. For 
\begin{align*}
    S^2 &= \over{n-2}\operatorname{SSE}\\
    &= \over{n-2}\pars{S_{yy}-\widehat{\beta_1}S_{xy}}\\
    &= \over{12-2}\pars{792 - \pfrac{-19}{60}(-1900)} = \frac{571}{30}
    \\ &\approx 19.0\overline{3}
\end{align*}

\disc Do we know how $S^2$ is distributed? This depends on our assumption on how $\varepsilon$ is distributed. 
$$y = \beta_0 + \beta_1 x + \varepsilon.$$
We have $\E*{\varepsilon} = 0$ and $\Var*{\varepsilon} = \sigma^2$, and thus are independent of $X$. 

\nl Note that the distribution of the point estimators $\widehat{\beta_0}$ and $\widehat{\beta_1}$ are $\sigma^2$ dependent (which we will estimate with $S^2$).

\nl However, the common assumption is that \say{everything} is normal! We assume $\varepsilon \sim \operatorname{N}(0, \sigma^2)$.

\nl We can now prove a Fisher's Theorem like result for the new $S^2$. 

\nl Namely\dots Theorem: $\displaystyle \frac{(n-2)S^2}{\sigma^2} = \frac{\operatorname{SSE}}{\sigma^2} \sim \chi^2(n-2)$, moreover $S^2$ is independent of $\widehat{\beta_0}$ and $\widehat{\beta_1}$. Proof: Omitted for mental health reasons. 