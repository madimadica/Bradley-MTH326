\section{T tests} %10.8
Recall that for a small sample $n < 30$. We need to use the $t$-distribution.

\nl Chapter 8: t-stat confidence interval $\displaystyle \Xbar \pm t_{\alpha/2}(\text{df})\sqrt{\frac{s^2}{n}}$

\example 100 mL sample of water from swimming areas are tested for fecal colliform bacteria. It is considered to be safe if the level of bacteria is less than 400 in 3.3 oz.

\nl 20 Samples are taken: Found $\Xbar = 1231$ and $S = 1038$.

\nl Construct a test to determine if it's safe to swim. Our test:
\begin{align*}
    &H_0 : \mu_0 = 400\\
    &H_a : \mu_a > 400
\end{align*}
Test statistic $\displaystyle T = \frac{\Xbar - \mu_0}{S \big/ \sqrt{n}} = \frac{1231 - 1038}{1038\big/\sqrt{20}} = 0.350$ is the test statistic.

\nl The degrees of freedom is $n - 1 = 19$. Using table 5, $t_{0.005}(19)=2.861$ The $p$-value is: 
\begin{align*}
    p &= \Pr(T \geq 3.580 \mid \mu_0 = 400)\\
    &< \Pr(T \geq 2.861)\\
    &= 0.005
\end{align*}

\nl Reject $H_0$: There is poop in the water; stay out!

\remark* In general:
$$\displaystyle H_a := \begin{cases} \mu > \mu_0 \\ \setBlue \mu < \mu_0 \\ \setRed \mu \neq \mu_0 \end{cases} \implies \text{RR} := \underbrace{\begin{cases}t > t_{\alpha} \\ \setBlue t < -t_{\alpha}\\ \setRed \abs{t} > t_{\alpha/2} \end{cases}}_{\textstyle \text{the } t \text{-stat}}$$

\example Lifestyle comparison. Monitoring the active time (in minutes per day) between 2 populations: obese and lean.
\begin{center}
    \begin{tabular}{ |c|c|c|c| } 
        \hline
        \textbf{Group} & \textbf{Count} & \textbf{Standing/Walking} & $S$\\
        \hline
        Obese & $n=10$ & 373.269 & 67.498\\ 
        \hline
        Lean & $n=13$ &  525.751 & 107.121\\ 
        \hline
      \end{tabular}
    \end{center}


\nl Question: Are these two groups significant significant? This is a \say{are population means different} question.
\begin{align*}
    H_0 &: \mu_L = \mu_0 \quad \implies \quad  \mu_L - \mu_0 = 0\\
    H_a &: \mu_L \neq \mu_0 \quad \implies \quad \mu_L - \mu_0 \neq 0
\end{align*}
Questions about \say{\setRed different} results in a 2-sided test

\nl Confidence interval: $\displaystyle \Xbar_1 - \Xbar_2 \pm t_{\alpha/2}(\df)\pars{S_p\sqrt{\dfrac{1}{n_1} + \dfrac{1}{n_2}}}$
$$\color{orange} S_{\text{pool}} \setBlack = \sqrt{\dfrac{(n_1-1){S_1}^2 + (n_2-1){S_2}^2}{n_1 + n_2 - 2}} \quad \text{with } \df = n_1 + n_2 - 2$$
The difference in means $t$ test statistic is 
$$T = \dfrac{\Xbar_1 - \Xbar_2 - (\mu_1 - \mu_2)}{S_p \sqrt{\over*{n_1} + \over*{n_2}}}$$
Our $\Xbar_L - \Xbar_0 = 152.482$. $H_0 : \mu_L - \mu_0 = 0$ is the null hypothesis. $\df = 10+13-2=21.$
\begin{align*}
    S_p &= \sqrt{\dfrac{(13-1)107.121^2 + (10-1)67.498^2 }{13+10-2}}\\
    &= 92.247.\\
    \\
    T &= \frac{152.481}{92.247 \sqrt{\over*{10} + \over*{13}}}\\
    &= 3.93
    \\\\p &= \P{|T|>3.93} \\ &= 2 \cdot \P{T > 3.93}\\ &< 2 \cdot 0.0005 \tag{by table 5}\\
    & = 0.001
\end{align*}
So $p$ is super small ($p < \alpha$) which implies that we reject the null hypothesis. That is, $\mu_0 < \mu_L$ is true.

\nnl Our goal this week is to justify hypothesis testing. 

\nl Last day: Pearson Neyman Lamma

\nl $\thru{X} \sim$ via PDF $f(x \mid \theta)$ when $\theta_0$ and $\theta_a$ are two possible values of $\theta$. If there exists $a$ the constant $k$ and a subset $C$ of the sample space such that
\begin{enumerate}
    \item $P\{(\thru{x}) \in C \mid \theta_0\} = \alpha$
    \item $\dfrac{L(\theta_0)}{L(\theta_a)} \leq k$ for $\thru{x} \in C$.
    \item $\dfrac{L(\theta_0)}{L(\theta_a)} \geq k$ for $\thru{x} \in \overline{C}$
\end{enumerate}
Then $C$ is a best critical region of size $\alpha$ for testing $H_0 : \theta = \theta_0$ versus $H_a : \theta = \theta_a$.

\nl Before the proof\dots example.

\example Let $\thru{Y}$ from $\displaystyle f(y \mid \theta) = \frac{2}{\theta}y e^{-y^2/\theta}$ and $y > 0$. The Rayleight distribution.

\nl Want to test  $H_0 : \theta = \theta_0$ versus $H_a : \theta = \theta_a$.

\nl Note $\displaystyle L(\thru{Y} \mid \theta) = \frac{2}{\theta} y_i e^{-y_i^2/\theta} \cdots \frac{2}{\theta} y_n e^{-y_n^2/\theta}$.
$$\pars{\frac{2}{\theta}}^n \underbrace{\exp \brac{- \pars{\sum_1^n y_i^2} \bigg/ \theta }}_{g(S,\theta)} \underbrace{\prod_1^n y_i}_{h}$$
We will come back to this and connect to sufficiency next day.

The N-P ratio of likelihood function becomes
\begin{align*}
    \dfrac{L(\theta_0)}{L(\theta_a)} &= 
    \frac
    {\pars{\frac{2}{\theta_0}}^n \exp \brac{- \pars{\sum_1^n y_i^2} \bigg/ \theta_0 }\prod_1^n y_i}
    {\pars{\frac{2}{\theta_a}}^n \exp \brac{- \pars{\sum_1^n y_i^2} \bigg/ \theta_a }\prod_1^n y_i}\\
    &= \pars{\frac{\theta_a}{\theta_0}}^n \exp \brac{-\sum y_i^2 \cdot \pars{\over{\theta_0} - \over{\theta_a}}}
\end{align*}
We want $C$ to relate to our rejection region RR.

\nl So (2), implies \say{reject $H_0$ if}
$$\pars{\frac{\theta_a}{\theta_0}}^n \exp \brac{-\sum y_i^2 \cdot \pars{\over{\theta_0} - \over{\theta_a}}} \leq k$$
This looks scary, but $\theta_a$ and $\theta_0$ are fixed and $\theta_0 < \theta_a$ so $\pars{\over{\theta_0} - \over{\theta_a}} > 0$. In the end, making $\dfrac{L(\theta_0)}{L(\theta_a)}$ \say{small enough} for $k$ simlifies does to the condition that $\sum_{i}^n y_i^2$ is large enough. i.e. we need $\sum_1^n y_i^2 > k'$ for some $k'$ (dependent upon $k$). 

\nl New problem: To determine an appropriate $k'$, we need to know how $S = \sum_1^n y_i^2$ is distributed. 

\nl Using CDF method ($\S$ 6.3) we can show that the distribution of $y^2$ is exponention with mean $\theta$. Then, via products of moment generating functions. $$S = \sum_1^n y_i^2 \sim \operatorname{Gamma}\pars{n, \over{\theta}}$$
Stop! What just happened. 

\nl The N-P lemma tells us that the most powerful test of $H_0$ vs $H_a$ is to use the statistic $S = \sum y_i^2$. Then, given any $\alpha$ level significance (5\%, 1\%, whatever), we use the test on $S$ to be if $S$ is larger than $100(1-\alpha)$ percentile of the $\operatorname{Gamma}\pars{n, \over{\theta}}$ distribution\dots we reject $H_0$. 

\nl Picture with some $\gamma_*$ such that $\int_{\gamma_*}^{\infty} = \alpha$.

\nl $S > \gamma_*$, we reject the null hypothesis. Moreover, by the NP lemme, we don't have to compare the powe rof other possible tests because the lemma says any data
$$(\thru{Y}) \in C \leftrightsquigarrow S \geq \gamma_*$$
$$C := \left\{ (\thru{Y}) \mid \sum y_i^2 \geq \gamma_* \right\}$$
which is itself a consequence of our significance level $\alpha$ results in the most powerful test. 
$$P(C \mid \theta_a) \geq P(D \mid \theta_a)$$
where $D$ is another critical region $(P(D \mid \theta_a) = \alpha )$.

\defn The test using the best critical region is called the most powerful test. 

\nl Recap: We wanted to construct a hypothesis test at $\alpha$ significance level. All in one fell swoop, the N-P lemme says
\begin{enumerate}
    \item Here is the stat to use ($S = \sum y_i^2$ in last example) 
    \item Here is your rejection region ($S \geq \gamma_a$ in last example)
    \item in fact, this is the most powerful test possible.
\end{enumerate}
(This is kinda awesome)

Theorem: NP lemma;
\begin{proof} (Continuous)\\
    Let $B \subseteq \R^n$ and define $\displaystyle \underbrace{\int_B L(\theta) = \int \cdots \int}_{n \text{times}} L(\thru{x} \mid \theta) \, dx_1\, dx_2, \dots, dx_n  $.\\
    Assume there exists a critical region $C$ satisfying bullets 1 2 and 3.

    \nl $\alpha$ fixed, $\alpha = \int_C L(\theta_0)$ (same as $P(C \mid \theta_0)$)

    \nl Need to prove \say{best}, i.e. $P(C \mid \theta_a) \geq P(D \mid \theta_a)$

    \nl Assume $D$ is another critical region,
    $$\alpha = \int_D L(\theta)_0 = P(D\mid \theta_0)$$
    So $\displaystyle 0 = \int_C L(\theta_0) - \int_D L(\theta_0) $
    \begin{align*}0 &= \int_{C \cap D^C} L(\theta_0) + \int_{C \cap D} L(\theta_0) - \brac{ \int_{C \cap D} L(\theta_0) + \int_{C^C \cap D} L(\theta_0) }\\
        &= \int_{C \cap D^C} L(\theta_0) - \int_{C^C \cap D} L(\theta_0)\\
    &\text{By (2), there exists $k$ such that $\dfrac{L(\theta_0)}{L(\theta_a)} \leq k$ i.e. $L(\theta_0) \leq kL(\theta_a)$ at every point in $C$.}\\
    & \leq k \int_{C \cap D^C} L(\theta_a) - \int_{C^C \cap D} L(\theta_0)\\
    &\text{By (3) there exists $k$ such that $\dfrac{L(\theta_0)}{L(\theta_a)} \geq k$ i.e.  $L(\theta_0) \geq kL(\theta_a)$}\\
    & \leq k \brac{\int_{C \cap D^C} L(\theta_a) - \int_{C^C \cap D} L(\theta_a)}\\
    &= k \brac{\int_{C \cap D^C} L(\theta_a) + \int_{C \cap D} L(\theta_a) - \brac{ \int_{C \cap D} L(\theta_a) + \int_{C^C \cap D} L(\theta_a) }}\\
    &= k \brac{\int_C L(\theta_a) - \int_D L(\theta_a)}\\
    & \implies \int_C L(\theta_a) - \int_D L(\theta_a) \geq 0\\
    & \implies \int_C L(\theta_a) \geq \int_D L(\theta_a)\\
    & \implies P(C \mid \theta_a) \geq P(D \mid \theta_a)
    \end{align*}
    Here, $C$ is a best critical region of size $\alpha$ by defininition.
\end{proof}
\example Back to our old sample size example. 
$$\thru{X} \sim \operatorname{N}(\mu,\,36)$$
We played with $H_0 : \mu = 50$ and $H_a : \mu = 55$. 

\nl Consider the ratio of the likelihood functions.

\begin{align*}
    \frac{L(50)}{L(55)}
    &= \dfrac{(72 \pi)^{-n/2} \exp \brac{-\pars{\over{72}} \sum_1^n (X_i - 50)^2} }{{(72 \pi)^{-n/2} \exp \brac{-\pars{\over{72}} \sum_1^n (X_i - 55)^2} }}\\
    &= \exp \brac{ - \over{72} \sum_1^n \brac{(X_i-50)^2-(X_i-55)^2}}\\
    &= \exp \brac{-\over{72} \sum_1^n (10X_i -525) }\\
    &= \exp \brac{-\frac{5}{36} \sum_1^n X_i + \frac{175n}{24}} \leq k \text{ to satisfy (2)}\\
    & \implies -\frac{5}{36} \sum_1^n X_i + \frac{175n}{24} \leq \ln k\\
    & \implies \sum_1^n X_i \geq \frac{105n}
    {2} - \frac{36}{5} \ln k
    \\ & \implies \Xbar = \over{n}\sum_1^n X_i \geq \frac{105}{2} - \frac{36\ln k}{5n}
\end{align*}
We have our stat $\Xbar \geq k'$ thus will define our rejectoin rejection.

    %\text{    By (2), there exists $k$ such that $\dfrac{L(\theta_0)}{L(\theta_a)} \leq k$ i.e. $L(\theta_0) \leq kL(\theta_a)$ at every point in $C$.

    %By (3) there exists $k$ such that $\dfrac{L(\theta_0)}{L(\theta_a)} \geq k$ i.e.  $L(\theta_0) \geq kL(\theta_a)$}




% ^  stuff to finish








